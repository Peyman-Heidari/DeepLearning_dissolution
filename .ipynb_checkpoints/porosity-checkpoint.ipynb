{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score\n",
    "from statistics import mean\n",
    "import usefulFuncs as uf\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('objs.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "    X_train, newxtest, y_train, y_validation_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test, X_validation, y_test, y_validation = train_test_split(newxtest,\\\n",
    "                                                            y_validation_test, test_size=0.5, random_state=1)\n",
    "\n",
    "y_train = y_train.tolist()\n",
    "y_validation = y_validation.tolist()\n",
    "y_test = y_test.tolist()\n",
    "\n",
    "y_train = np.reshape(y_train, (len(y_train),1))\n",
    "y_validation = np.reshape(y_validation, (len(y_validation),1))\n",
    "y_test = np.reshape(y_test, (len(y_test),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Rsquared(reality1,prediction1):\n",
    "    y_pred = np.array(prediction1)\n",
    "    y_true = np.array(reality1)\n",
    "    u = ((y_true - y_pred) ** 2).sum() \n",
    "    v = ((y_true - y_true.mean()) ** 2).sum()\n",
    "    Rs = 1 - (u/v)\n",
    "    return Rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "number_of_data = len(X_train)\n",
    "number_of_features = len(X_train[0])\n",
    "number_of_hidden_1 = 64\n",
    "number_of_hidden_2 = 128\n",
    "gamma = 0.05\n",
    "batch_size = 128\n",
    "best_case_R2 =0.887\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    \n",
    "    tf_train_dataset = tf.constant(X_train, dtype=tf.float32)\n",
    "    tf_train_labels = tf.constant(y_train, dtype=tf.float32)\n",
    "    #tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, number_of_features))\n",
    "    #tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size,1))\n",
    "    tf_valid_dataset = tf.constant(X_validation, dtype=tf.float32)\n",
    "    tf_valid_labels = tf.constant(y_validation, dtype=tf.float32)\n",
    "    tf_test_dataset = tf.constant(X_test, dtype=tf.float32)\n",
    "    tf_test_labels = tf.constant(y_test, dtype=tf.float32)\n",
    "  \n",
    "    # Variables.\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random values following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([number_of_features, number_of_hidden_1], dtype=tf.float32, seed=123))\n",
    "    biases1 = tf.Variable(tf.zeros([number_of_hidden_1], dtype=tf.float32))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([number_of_hidden_1, number_of_hidden_1], dtype=tf.float32, seed=123))\n",
    "    biases2 = tf.Variable(tf.zeros([number_of_hidden_1], dtype=tf.float32))\n",
    "    \n",
    "    weights3 = tf.Variable(tf.truncated_normal([number_of_hidden_1, number_of_hidden_1], dtype=tf.float32, seed=123))\n",
    "    biases3 = tf.Variable(tf.zeros([number_of_hidden_1], dtype=tf.float32))\n",
    "    \n",
    "    weights4 = tf.Variable(tf.truncated_normal([number_of_hidden_1, number_of_hidden_2], dtype=tf.float32, seed=123))\n",
    "    biases4 = tf.Variable(tf.zeros([number_of_hidden_2], dtype=tf.float32))\n",
    "    \n",
    "    weights5 = tf.Variable(tf.truncated_normal([number_of_hidden_2, number_of_hidden_2], dtype=tf.float32, seed=123))\n",
    "    biases5 = tf.Variable(tf.zeros([number_of_hidden_2], dtype=tf.float32))\n",
    "    \n",
    "    weights6 = tf.Variable(tf.truncated_normal([number_of_hidden_2, number_of_hidden_2], dtype=tf.float32, seed=123))\n",
    "    biases6 = tf.Variable(tf.zeros([number_of_hidden_2], dtype=tf.float32))\n",
    "    \n",
    "    weights7 = tf.Variable(tf.truncated_normal([number_of_hidden_2, number_of_hidden_2], dtype=tf.float32, seed=123))\n",
    "    biases7 = tf.Variable(tf.zeros([number_of_hidden_2], dtype=tf.float32))\n",
    "    \n",
    "    weights8 = tf.Variable(tf.truncated_normal([number_of_hidden_2, number_of_hidden_2], dtype=tf.float32, seed=123))\n",
    "    biases8 = tf.Variable(tf.zeros([number_of_hidden_2], dtype=tf.float32))\n",
    "    \n",
    "    weights9 = tf.Variable(tf.truncated_normal([number_of_hidden_2, number_of_hidden_2], dtype=tf.float32, seed=123))\n",
    "    biases9 = tf.Variable(tf.zeros([number_of_hidden_2], dtype=tf.float32))\n",
    "    \n",
    "    weights10 = tf.Variable(tf.truncated_normal([number_of_hidden_2, number_of_hidden_2], dtype=tf.float32, seed=123))\n",
    "    biases10 = tf.Variable(tf.zeros([number_of_hidden_2], dtype=tf.float32))\n",
    "    \n",
    "    weights11 = tf.Variable(tf.truncated_normal([number_of_hidden_2, number_of_hidden_2], dtype=tf.float32, seed=123))\n",
    "    biases11 = tf.Variable(tf.zeros([number_of_hidden_2], dtype=tf.float32))\n",
    "    \n",
    "    weights12 = tf.Variable(tf.truncated_normal([number_of_hidden_2, number_of_hidden_2], dtype=tf.float32, seed=123))\n",
    "    biases12 = tf.Variable(tf.zeros([number_of_hidden_2], dtype=tf.float32))\n",
    "    \n",
    "    weights13 = tf.Variable(tf.truncated_normal([number_of_hidden_2, number_of_hidden_2], dtype=tf.float32, seed=123))\n",
    "    biases13 = tf.Variable(tf.zeros([number_of_hidden_2], dtype=tf.float32))\n",
    "    \n",
    "    weights14 = tf.Variable(tf.truncated_normal([number_of_hidden_2, number_of_hidden_2], dtype=tf.float32, seed=123))\n",
    "    biases14 = tf.Variable(tf.zeros([number_of_hidden_2], dtype=tf.float32))\n",
    "    \n",
    "    weights_out = tf.Variable(tf.truncated_normal([number_of_hidden_2, 1], dtype=tf.float32, seed=123))\n",
    "    biase_out = tf.Variable(tf.zeros([1], dtype=tf.float32))\n",
    "    \n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    def R2(y_true,y_pred):\n",
    "        u = tf.reduce_sum(tf.square(tf.sub(y_true, y_pred)))\n",
    "        v = tf.reduce_sum(tf.square(tf.sub(y_true, tf.reduce_mean(y_true))))\n",
    "        Rs = 1 - tf.divide(u,v)\n",
    "        return Rs\n",
    "    \n",
    "    def model(data):\n",
    "        multiplications1 = tf.matmul(data, weights1) + biases1\n",
    "        multiplications2 = tf.matmul(multiplications1, weights2) + biases2\n",
    "        multiplications3 = tf.matmul(multiplications2, weights3) + biases3\n",
    "        multiplications4 = tf.matmul(multiplications3, weights4) + biases4\n",
    "        multiplications5 = (tf.matmul(multiplications4, weights5) + biases5)\n",
    "        multiplications6 = (tf.matmul(multiplications5, weights6) + biases6)\n",
    "        multiplications7 = (tf.matmul(multiplications6, weights7) + biases7)\n",
    "        multiplications8 = (tf.matmul(multiplications7, weights8) + biases8)\n",
    "        multiplications9 = (tf.matmul(multiplications8, weights9) + biases9)\n",
    "        multiplications10 = (tf.matmul(multiplications9, weights10) + biases10)\n",
    "        multiplications11 = tf.tanh(tf.matmul(multiplications10, weights11) + biases11)\n",
    "        multiplications12 = tf.tanh(tf.matmul(multiplications11, weights12) + biases12)\n",
    "        multiplications13 = tf.sigmoid(tf.matmul(multiplications12, weights13) + biases13)\n",
    "        multiplications14 = tf.sigmoid(tf.matmul(multiplications13, weights14) + biases14)\n",
    "        output = tf.matmul(multiplications14, weights_out)+biase_out\n",
    "        return output\n",
    "    # Training computation.\n",
    "    # We multiply the inputs with the weight matrix, and add biases. \n",
    "    output = model(tf_train_dataset)\n",
    "    loss = tf.reduce_sum(tf.square(tf.sub(output,tf_train_labels)))\n",
    "                            #+ gamma*tf.reduce_mean(tf.nn.l2_loss(weights_out))+\\\n",
    "                            #gamma*tf.reduce_mean(tf.nn.l2_loss(weights14))\n",
    "                            #gamma*tf.reduce_mean(tf.nn.l2_loss(weights3))+\\\n",
    "                            #gamma*tf.reduce_mean(tf.nn.l2_loss(weights4))+\\\n",
    "                            #gamma*tf.reduce_mean(tf.nn.l2_loss(weights_out))  \n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    \n",
    "    #learning_rate = tf.train.exponential_decay(0.001, global_step, 100000, 0.96)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    #optimizer = tf.train.MOmentumOptimizer(0.01, 0.2).minimize(loss)\n",
    "    \n",
    "    #optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "    optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # These are not part of training, but merely here so that we can report\n",
    "    # accuracy figures as we train.\n",
    "    train_prediction = model(tf_train_dataset)\n",
    "    valid_prediction = model(tf_valid_dataset)\n",
    "    test_prediction = model(tf_test_dataset)\n",
    "    train_r2 = R2(tf_train_labels,train_prediction)\n",
    "    valid_r2 = R2(tf_valid_labels,valid_prediction)\n",
    "    test_r2 = R2(tf_test_labels,test_prediction)\n",
    "    variables_out ={'w1':weights1,'w2':weights2, 'w3':weights3,  'w4':weights4, 'w5':weights5,'w6':weights6,\\\n",
    "                    'w7':weights7,'w8':weights8,'w9':weights9,'w10':weights10,'w11':weights11,'w12':weights12,\\\n",
    "                    'w13':weights13,'w14':weights14, 'wo':weights_out,'b1':biases1,'b2':biases2,'b3':biases3,\\\n",
    "                    'b4':biases4,'b5':biases5,'b6':biases6,'b7':biases7,'b8':biases8, 'b9':biases9, 'b10':biases10,\\\n",
    "                    'b11':biases11,'b12':biases12, 'b13':biases13, 'b14':biases14,'bo':biase_out,\\\n",
    "                    'train_pred': train_prediction,'valid_pred': valid_prediction,'test_pred': test_prediction,\\\n",
    "                    'train_r2': train_r2, 'valid_r2': valid_r2, 'test_r2': test_r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 285420.406250\n",
      "Training R2:  -860367668.254\n",
      "Validation R2:  -233040523.297\n",
      "================================\n",
      "++++++++++++++++++++++\n",
      "New Best R2:  0.888021\n",
      "Train R2:  0.924431\n",
      "Test R2:  0.890301\n",
      "++++++++++++++++++++++\n",
      "[[ 0.00107961]\n",
      " [ 0.00141432]\n",
      " [ 0.00140901]\n",
      " [ 0.00130805]\n",
      " [ 0.00158214]\n",
      " [ 0.00156346]\n",
      " [ 0.00218049]\n",
      " [ 0.00151734]\n",
      " [ 0.00145826]\n",
      " [ 0.00148   ]\n",
      " [ 0.00221004]\n",
      " [ 0.00149338]\n",
      " [ 0.00156723]\n",
      " [ 0.00059393]\n",
      " [ 0.00113733]\n",
      " [ 0.00154738]\n",
      " [ 0.00221742]\n",
      " [ 0.00199316]\n",
      " [ 0.00154598]\n",
      " [ 0.00158017]\n",
      " [ 0.00157203]\n",
      " [ 0.00149338]\n",
      " [ 0.00173535]\n",
      " [ 0.00214568]\n",
      " [ 0.00148797]\n",
      " [ 0.00152705]\n",
      " [ 0.00173634]\n",
      " [ 0.00149869]\n",
      " [ 0.00077364]\n",
      " [ 0.00151725]\n",
      " [ 0.00170441]\n",
      " [ 0.00156725]\n",
      " [ 0.0006987 ]\n",
      " [ 0.00077643]\n",
      " [ 0.00171262]\n",
      " [ 0.00156271]\n",
      " [ 0.00092632]\n",
      " [ 0.002062  ]\n",
      " [ 0.00124596]\n",
      " [ 0.0014365 ]\n",
      " [ 0.00118547]\n",
      " [ 0.00130805]\n",
      " [ 0.00217101]\n",
      " [ 0.00077643]\n",
      " [ 0.00129775]\n",
      " [ 0.00200965]\n",
      " [ 0.00131322]\n",
      " [ 0.00172981]\n",
      " [ 0.00154937]\n",
      " [ 0.00153622]\n",
      " [ 0.00118547]\n",
      " [ 0.0015272 ]\n",
      " [ 0.00155845]\n",
      " [ 0.00044077]\n",
      " [ 0.00145147]\n",
      " [ 0.00138303]\n",
      " [ 0.00156502]\n",
      " [ 0.00155475]\n",
      " [ 0.00155219]\n",
      " [ 0.00155219]\n",
      " [ 0.00115068]\n",
      " [ 0.00170536]\n",
      " [ 0.00128134]\n",
      " [ 0.00055495]\n",
      " [ 0.00124596]\n",
      " [ 0.00154096]\n",
      " [ 0.00111453]\n",
      " [ 0.00153281]\n",
      " [ 0.00156925]\n",
      " [ 0.00138827]\n",
      " [ 0.00124596]\n",
      " [ 0.00144942]\n",
      " [ 0.00077643]\n",
      " [ 0.0015634 ]\n",
      " [ 0.00157515]\n",
      " [ 0.00147824]\n",
      " [ 0.00155443]\n",
      " [ 0.00153216]\n",
      " [ 0.0018481 ]\n",
      " [ 0.00157371]\n",
      " [ 0.00140901]\n",
      " [ 0.00158214]\n",
      " [ 0.00127793]\n",
      " [ 0.00172221]\n",
      " [ 0.00174458]\n",
      " [ 0.00154937]\n",
      " [ 0.00157018]\n",
      " [ 0.00156701]\n",
      " [ 0.00203464]\n",
      " [ 0.00120266]\n",
      " [ 0.00220698]\n",
      " [ 0.00155664]\n",
      " [ 0.00168187]\n",
      " [ 0.00151725]\n",
      " [ 0.00118278]\n",
      " [ 0.00221311]\n",
      " [ 0.00198732]\n",
      " [ 0.00137258]\n",
      " [ 0.00154738]\n",
      " [ 0.00133733]\n",
      " [ 0.00156713]\n",
      " [ 0.00075276]\n",
      " [ 0.00152705]\n",
      " [ 0.0014856 ]\n",
      " [ 0.00147952]\n",
      " [ 0.00113733]\n",
      " [ 0.00171262]\n",
      " [ 0.00077364]\n",
      " [ 0.00157403]\n",
      " [ 0.00149869]\n",
      " [ 0.00160279]\n",
      " [ 0.00148208]\n",
      " [ 0.00153196]\n",
      " [ 0.00150855]\n",
      " [ 0.00156478]\n",
      " [ 0.0014035 ]\n",
      " [ 0.00086279]\n",
      " [ 0.00174072]\n",
      " [ 0.00221995]\n",
      " [ 0.00156626]\n",
      " [ 0.00196251]\n",
      " [ 0.00090657]\n",
      " [ 0.00131322]\n",
      " [ 0.00149387]\n",
      " [ 0.00146549]\n",
      " [ 0.00154821]\n",
      " [ 0.00191959]\n",
      " [ 0.00140901]\n",
      " [ 0.00197233]\n",
      " [ 0.00194511]\n",
      " [ 0.0005436 ]\n",
      " [ 0.00147824]\n",
      " [ 0.00156728]\n",
      " [ 0.00156728]\n",
      " [ 0.00218143]\n",
      " [ 0.0015634 ]\n",
      " [ 0.00146349]\n",
      " [ 0.00158017]\n",
      " [ 0.00220888]\n",
      " [ 0.00171064]\n",
      " [ 0.00194703]\n",
      " [ 0.00147952]\n",
      " [ 0.00172673]\n",
      " [ 0.00174468]\n",
      " [ 0.00153908]\n",
      " [ 0.00157414]\n",
      " [ 0.0015634 ]\n",
      " [ 0.00201488]\n",
      " [ 0.00134547]\n",
      " [ 0.00137885]\n",
      " [ 0.00120012]\n",
      " [ 0.00138303]\n",
      " [ 0.00157107]\n",
      " [ 0.00181949]\n",
      " [ 0.00173983]\n",
      " [ 0.00074921]\n",
      " [ 0.00174807]\n",
      " [ 0.00071752]\n",
      " [ 0.00059393]\n",
      " [ 0.00216891]\n",
      " [ 0.00157107]\n",
      " [ 0.00220175]\n",
      " [ 0.00218852]\n",
      " [ 0.00112838]\n",
      " [ 0.00157107]\n",
      " [ 0.00201677]\n",
      " [ 0.00152365]\n",
      " [ 0.00151715]\n",
      " [ 0.00158017]\n",
      " [ 0.00157072]\n",
      " [ 0.00118278]\n",
      " [ 0.00154738]\n",
      " [ 0.0020127 ]\n",
      " [ 0.00089501]\n",
      " [ 0.0015276 ]\n",
      " [ 0.0014856 ]\n",
      " [ 0.00118278]\n",
      " [ 0.00075276]\n",
      " [ 0.00153604]\n",
      " [ 0.00148   ]\n",
      " [ 0.00135405]\n",
      " [ 0.00171452]\n",
      " [ 0.00138303]\n",
      " [ 0.0015211 ]\n",
      " [ 0.00214681]\n",
      " [ 0.00171581]\n",
      " [ 0.00153216]\n",
      " [ 0.00171064]\n",
      " [ 0.00157556]\n",
      " [ 0.00157547]\n",
      " [ 0.00152365]\n",
      " [ 0.00141771]\n",
      " [ 0.0015772 ]\n",
      " [ 0.00113733]\n",
      " [ 0.00218049]\n",
      " [ 0.00133879]\n",
      " [ 0.00172275]\n",
      " [ 0.00147403]\n",
      " [ 0.00173911]\n",
      " [ 0.00218938]\n",
      " [ 0.00086279]\n",
      " [ 0.00197233]\n",
      " [ 0.00156837]\n",
      " [ 0.00156271]\n",
      " [ 0.00126464]\n",
      " [ 0.00221061]\n",
      " [ 0.00137258]\n",
      " [ 0.00156837]\n",
      " [ 0.00220445]\n",
      " [ 0.00170606]\n",
      " [ 0.00153487]\n",
      " [ 0.00156736]\n",
      " [ 0.00068816]\n",
      " [ 0.0006987 ]\n",
      " [ 0.00115068]\n",
      " [ 0.00151707]\n",
      " [ 0.0015498 ]\n",
      " [ 0.00052532]\n",
      " [ 0.00072095]\n",
      " [ 0.00171581]\n",
      " [ 0.00156744]\n",
      " [ 0.00159209]\n",
      " [ 0.00086279]\n",
      " [ 0.00170913]\n",
      " [ 0.00075276]\n",
      " [ 0.00156614]\n",
      " [ 0.0015764 ]\n",
      " [ 0.0017303 ]\n",
      " [ 0.00058686]\n",
      " [ 0.00152855]\n",
      " [ 0.00173983]\n",
      " [ 0.00194245]\n",
      " [ 0.00186574]\n",
      " [ 0.00171581]\n",
      " [ 0.00153728]\n",
      " [ 0.00157682]\n",
      " [ 0.00201393]\n",
      " [ 0.00155845]\n",
      " [ 0.00075276]\n",
      " [ 0.00211227]\n",
      " [ 0.00171579]\n",
      " [ 0.00169725]\n",
      " [ 0.00218852]\n",
      " [ 0.00106646]\n",
      " [ 0.00156802]\n",
      " [ 0.00055495]\n",
      " [ 0.00157177]\n",
      " [ 0.00157071]\n",
      " [ 0.00156626]\n",
      " [ 0.00218789]\n",
      " [ 0.00111453]\n",
      " [ 0.00155475]\n",
      " [ 0.00151936]\n",
      " [ 0.00131588]\n",
      " [ 0.00124596]\n",
      " [ 0.0015276 ]\n",
      " [ 0.00126464]\n",
      " [ 0.00155845]\n",
      " [ 0.00156614]\n",
      " [ 0.00148387]\n",
      " [ 0.00156736]\n",
      " [ 0.00154826]\n",
      " [ 0.00141432]\n",
      " [ 0.00141432]\n",
      " [ 0.00157203]\n",
      " [ 0.00175546]\n",
      " [ 0.00090657]\n",
      " [ 0.00156866]\n",
      " [ 0.00156925]\n",
      " [ 0.00075905]\n",
      " [ 0.00146716]\n",
      " [ 0.00156626]\n",
      " [ 0.00062889]\n",
      " [ 0.00129775]\n",
      " [ 0.00144942]\n",
      " [ 0.00172893]\n",
      " [ 0.00215806]\n",
      " [ 0.00172442]\n",
      " [ 0.00157682]\n",
      " [ 0.00156478]\n",
      " [ 0.00157203]\n",
      " [ 0.00221108]\n",
      " [ 0.00142284]\n",
      " [ 0.00147403]\n",
      " [ 0.00198243]\n",
      " [ 0.00031747]\n",
      " [ 0.00152282]\n",
      " [ 0.00154631]\n",
      " [ 0.00174458]\n",
      " [ 0.00103141]\n",
      " [ 0.00219483]\n",
      " [ 0.00181132]\n",
      " [ 0.00149387]\n",
      " [ 0.0015634 ]\n",
      " [ 0.00156502]\n",
      " [ 0.00179538]\n",
      " [ 0.00156775]\n",
      " [ 0.00138483]\n",
      " [ 0.00148387]\n",
      " [ 0.00155817]\n",
      " [ 0.00029768]\n",
      " [ 0.00116332]\n",
      " [ 0.00045015]\n",
      " [ 0.00174747]\n",
      " [ 0.00154319]\n",
      " [ 0.00220698]\n",
      " [ 0.00115819]\n",
      " [ 0.00155341]\n",
      " [ 0.00164424]\n",
      " [ 0.00130805]\n",
      " [ 0.00116332]\n",
      " [ 0.00196362]\n",
      " [ 0.00173663]\n",
      " [ 0.00142271]\n",
      " [ 0.00163139]\n",
      " [ 0.00080798]\n",
      " [ 0.00154096]\n",
      " [ 0.00170844]\n",
      " [ 0.00080798]\n",
      " [ 0.00174307]\n",
      " [ 0.0015772 ]\n",
      " [ 0.00047471]\n",
      " [ 0.00170441]\n",
      " [ 0.001736  ]\n",
      " [ 0.00144942]\n",
      " [ 0.00156775]\n",
      " [ 0.00220816]\n",
      " [ 0.00157274]\n",
      " [ 0.00047471]\n",
      " [ 0.00156006]\n",
      " [ 0.00217101]\n",
      " [ 0.00137885]\n",
      " [ 0.0015498 ]\n",
      " [ 0.00174379]\n",
      " [ 0.00145147]\n",
      " [ 0.00173965]\n",
      " [ 0.00173911]\n",
      " [ 0.00149338]\n",
      " [ 0.00153216]\n",
      " [ 0.00155341]\n",
      " [ 0.00171064]\n",
      " [ 0.00029768]\n",
      " [ 0.00145826]\n",
      " [ 0.00134547]\n",
      " [ 0.00045015]\n",
      " [ 0.00114769]\n",
      " [ 0.00135137]\n",
      " [ 0.00151936]\n",
      " [ 0.00157744]\n",
      " [ 0.001112  ]\n",
      " [ 0.00156736]\n",
      " [ 0.00157371]\n",
      " [ 0.00152855]\n",
      " [ 0.00221749]\n",
      " [ 0.00075905]\n",
      " [ 0.00155816]\n",
      " [ 0.00156744]\n",
      " [ 0.00157274]\n",
      " [ 0.00173907]\n",
      " [ 0.00216969]\n",
      " [ 0.00153728]\n",
      " [ 0.0015634 ]\n",
      " [ 0.00117059]\n",
      " [ 0.00157371]\n",
      " [ 0.0015634 ]\n",
      " [ 0.00080798]\n",
      " [ 0.00169289]\n",
      " [ 0.00156448]\n",
      " [ 0.00134547]\n",
      " [ 0.00157004]\n",
      " [ 0.00045015]\n",
      " [ 0.00149155]\n",
      " [ 0.00154821]\n",
      " [ 0.00153963]\n",
      " [ 0.00156708]\n",
      " [ 0.0014133 ]\n",
      " [ 0.00143548]\n",
      " [ 0.00156599]\n",
      " [ 0.00137258]\n",
      " [ 0.00118278]\n",
      " [ 0.00154821]\n",
      " [ 0.00148387]\n",
      " [ 0.00221748]\n",
      " [ 0.00169622]\n",
      " [ 0.00071732]\n",
      " [ 0.00173535]\n",
      " [ 0.00218282]\n",
      " [ 0.00147403]\n",
      " [ 0.00157644]\n",
      " [ 0.00094401]\n",
      " [ 0.00094401]\n",
      " [ 0.00099847]\n",
      " [ 0.00077364]\n",
      " [ 0.00164424]\n",
      " [ 0.00221108]\n",
      " [ 0.00169289]\n",
      " [ 0.00180967]\n",
      " [ 0.00111453]\n",
      " [ 0.00151051]\n",
      " [ 0.00155341]\n",
      " [ 0.00172449]\n",
      " [ 0.00158309]\n",
      " [ 0.00149387]\n",
      " [ 0.00152855]\n",
      " [ 0.00154631]\n",
      " [ 0.00173495]\n",
      " [ 0.00172217]\n",
      " [ 0.00179538]\n",
      " [ 0.00148098]\n",
      " [ 0.00155007]\n",
      " [ 0.00171231]\n",
      " [ 0.00075905]\n",
      " [ 0.00075276]\n",
      " [ 0.00218337]\n",
      " [ 0.00190745]\n",
      " [ 0.00157902]\n",
      " [ 0.0013444 ]\n",
      " [ 0.00164424]\n",
      " [ 0.00157333]\n",
      " [ 0.00158338]\n",
      " [ 0.00077364]\n",
      " [ 0.00156006]\n",
      " [ 0.00126168]\n",
      " [ 0.00151051]\n",
      " [ 0.00148387]\n",
      " [ 0.0015634 ]\n",
      " [ 0.00048587]\n",
      " [ 0.00156713]\n",
      " [ 0.00121756]\n",
      " [ 0.00168475]\n",
      " [ 0.00152282]\n",
      " [ 0.00157976]\n",
      " [ 0.00029768]\n",
      " [ 0.00107961]\n",
      " [ 0.00155076]\n",
      " [ 0.00157018]\n",
      " [ 0.00130805]\n",
      " [ 0.00135405]\n",
      " [ 0.00221004]\n",
      " [ 0.00156203]]\n",
      "Loss at step 1000: 0.000024\n",
      "Training R2:  0.926176730089\n",
      "Validation R2:  0.88767709284\n",
      "================================\n",
      "Loss at step 2000: 0.000024\n",
      "Training R2:  0.926171719825\n",
      "Validation R2:  0.887716654127\n",
      "================================\n",
      "Loss at step 3000: 0.000024\n",
      "Training R2:  0.926156030478\n",
      "Validation R2:  0.888015289719\n",
      "================================\n",
      "Loss at step 4000: 0.000033\n",
      "Training R2:  0.901368831852\n",
      "Validation R2:  0.879613899515\n",
      "================================\n",
      "Loss at step 5000: 0.000024\n",
      "Training R2:  0.926176128032\n",
      "Validation R2:  0.887667420519\n",
      "================================\n",
      "Loss at step 6000: 0.000024\n",
      "Training R2:  0.926170459087\n",
      "Validation R2:  0.887689774251\n",
      "================================\n",
      "Test R2:  0.893028895112\n",
      "[[ 0.00142948]\n",
      " [ 0.00042191]\n",
      " [ 0.00152086]\n",
      " ..., \n",
      " [ 0.00146326]\n",
      " [ 0.00157324]\n",
      " [ 0.00157212]]\n",
      "+++++++++++++\n",
      "[[ 0.0014196]\n",
      " [ 0.0004934]\n",
      " [ 0.0015292]\n",
      " ..., \n",
      " [ 0.0014644]\n",
      " [ 0.0015723]\n",
      " [ 0.0015703]]\n"
     ]
    }
   ],
   "source": [
    "num_steps = 6001\n",
    "\n",
    "with tf.Session(graph=graph, config=tf.ConfigProto(log_device_placement=True)) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases. \n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        #offset = (step * batch_size) % (number_of_data - batch_size)\n",
    "        #batch_data = X_train[offset:(offset + batch_size), :]\n",
    "        #batch_labels = y_train[offset:(offset + batch_size)]\n",
    "        #batch_labels = np.reshape(batch_labels, (batch_size,1))\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        #feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions, train_R2,valid_R2, test_R2,Test_prediction, variable_dict = session.run([optimizer, loss,\\\n",
    "                                                                           train_prediction, train_r2,valid_r2,\\\n",
    "                                                                                    test_r2, test_prediction, variables_out])\n",
    "\n",
    "        if (valid_R2-best_case_R2) > 0.001 :\n",
    "            best_case_R2 = valid_R2\n",
    "            print('++++++++++++++++++++++')\n",
    "            print ('New Best R2: ',best_case_R2)\n",
    "            print ('Train R2: ',train_R2)\n",
    "            print ('Test R2: ',test_R2)\n",
    "            print('++++++++++++++++++++++')\n",
    "            print(Test_prediction)\n",
    "            file_name = 'variables'+str(step)+'.pickle'\n",
    "            test_file_name = 'test'+str(step)+'.pickle'\n",
    "            pickle.dump(variable_dict, open(file_name, 'wb'))\n",
    "            \n",
    "        if (step % 1000 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training R2: ',Rsquared(y_train, predictions))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph\n",
    "            # dependencies.\n",
    "            print('Validation R2: ',Rsquared( y_validation,valid_prediction.eval()))\n",
    "            print('================================')\n",
    "    print('Test R2: ', Rsquared(y_test, test_prediction.eval()))\n",
    "    print (predictions)\n",
    "    print ('+++++++++++++')\n",
    "    print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
