{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score\n",
    "from statistics import mean\n",
    "import usefulFuncs as uf\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('objs.pickle','rb') as f:  # Python 3: open(..., 'rb')\n",
    "    X_train, newxtest, y_train, y_validation_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test, X_validation, y_test, y_validation = train_test_split(newxtest,\\\n",
    "                                                            y_validation_test, test_size=0.5, random_state=1)\n",
    "\n",
    "y_train = y_train.tolist()\n",
    "y_validation = y_validation.tolist()\n",
    "y_test = y_test.tolist()\n",
    "\n",
    "y_train = np.reshape(y_train, (len(y_train),1))\n",
    "y_validation = np.reshape(y_validation, (len(y_validation),1))\n",
    "y_test = np.reshape(y_test, (len(y_test),1))\n",
    "\n",
    "np.savetxt('y_test.txt', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Rsquared(reality1,prediction1):\n",
    "    y_pred = np.array(prediction1)\n",
    "    y_true = np.array(reality1)\n",
    "    u = ((y_true - y_pred) ** 2).sum() \n",
    "    v = ((y_true - y_true.mean()) ** 2).sum()\n",
    "    Rs = 1 - (u/v)\n",
    "    return Rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "number_of_data = len(X_train)\n",
    "number_of_features = len(X_train[0])\n",
    "number_of_hidden_1 = 64\n",
    "number_of_hidden_2 = 128\n",
    "gamma = 0.05\n",
    "batch_size = 128\n",
    "best_case_R2 =0.88\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    \n",
    "    tf_train_dataset = tf.constant(X_train, dtype=tf.float32)\n",
    "    tf_train_labels = tf.constant(y_train, dtype=tf.float32)\n",
    "    #tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, number_of_features))\n",
    "    #tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size,1))\n",
    "    tf_valid_dataset = tf.constant(X_validation, dtype=tf.float32)\n",
    "    tf_valid_labels = tf.constant(y_validation, dtype=tf.float32)\n",
    "    tf_test_dataset = tf.constant(X_test, dtype=tf.float32)\n",
    "    tf_test_labels = tf.constant(y_test, dtype=tf.float32)\n",
    "  \n",
    "    # Variables.\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random values following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([number_of_features, number_of_hidden_1], dtype=tf.float32, seed=123))\n",
    "    biases1 = tf.Variable(tf.zeros([number_of_hidden_1], dtype=tf.float32))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([number_of_hidden_1, number_of_hidden_1], dtype=tf.float32, seed=123))\n",
    "    biases2 = tf.Variable(tf.zeros([number_of_hidden_1], dtype=tf.float32))\n",
    "    \n",
    "    weights3 = tf.Variable(tf.truncated_normal([number_of_hidden_1, number_of_hidden_1], dtype=tf.float32, seed=123))\n",
    "    biases3 = tf.Variable(tf.zeros([number_of_hidden_1], dtype=tf.float32))\n",
    "    \n",
    "    weights4 = tf.Variable(tf.truncated_normal([number_of_hidden_1, number_of_hidden_2], dtype=tf.float32, seed=123))\n",
    "    biases4 = tf.Variable(tf.zeros([number_of_hidden_2], dtype=tf.float32))\n",
    "    \n",
    "    weights5 = tf.Variable(tf.truncated_normal([number_of_hidden_2, number_of_hidden_2], dtype=tf.float32, seed=123))\n",
    "    biases5 = tf.Variable(tf.zeros([number_of_hidden_2], dtype=tf.float32))\n",
    "    \n",
    "    weights6 = tf.Variable(tf.truncated_normal([number_of_hidden_2, number_of_hidden_2], dtype=tf.float32, seed=123))\n",
    "    biases6 = tf.Variable(tf.zeros([number_of_hidden_2], dtype=tf.float32))\n",
    "    \n",
    "    weights7 = tf.Variable(tf.truncated_normal([number_of_hidden_2, number_of_hidden_2], dtype=tf.float32, seed=123))\n",
    "    biases7 = tf.Variable(tf.zeros([number_of_hidden_2], dtype=tf.float32))\n",
    "    \n",
    "    weights8 = tf.Variable(tf.truncated_normal([number_of_hidden_2, number_of_hidden_2], dtype=tf.float32, seed=123))\n",
    "    biases8 = tf.Variable(tf.zeros([number_of_hidden_2], dtype=tf.float32))\n",
    "    \n",
    "    weights9 = tf.Variable(tf.truncated_normal([number_of_hidden_2, number_of_hidden_2], dtype=tf.float32, seed=123))\n",
    "    biases9 = tf.Variable(tf.zeros([number_of_hidden_2], dtype=tf.float32))\n",
    "    \n",
    "    weights10 = tf.Variable(tf.truncated_normal([number_of_hidden_2, number_of_hidden_2], dtype=tf.float32, seed=123))\n",
    "    biases10 = tf.Variable(tf.zeros([number_of_hidden_2], dtype=tf.float32))\n",
    "    \n",
    "    weights11 = tf.Variable(tf.truncated_normal([number_of_hidden_2, number_of_hidden_2], dtype=tf.float32, seed=123))\n",
    "    biases11 = tf.Variable(tf.zeros([number_of_hidden_2], dtype=tf.float32))\n",
    "    \n",
    "    weights12 = tf.Variable(tf.truncated_normal([number_of_hidden_2, number_of_hidden_2], dtype=tf.float32, seed=123))\n",
    "    biases12 = tf.Variable(tf.zeros([number_of_hidden_2], dtype=tf.float32))\n",
    "    \n",
    "    weights13 = tf.Variable(tf.truncated_normal([number_of_hidden_2, number_of_hidden_2], dtype=tf.float32, seed=123))\n",
    "    biases13 = tf.Variable(tf.zeros([number_of_hidden_2], dtype=tf.float32))\n",
    "    \n",
    "    weights14 = tf.Variable(tf.truncated_normal([number_of_hidden_2, number_of_hidden_2], dtype=tf.float32, seed=123))\n",
    "    biases14 = tf.Variable(tf.zeros([number_of_hidden_2], dtype=tf.float32))\n",
    "    \n",
    "    weights_out = tf.Variable(tf.truncated_normal([number_of_hidden_2, 1], dtype=tf.float32, seed=123))\n",
    "    biase_out = tf.Variable(tf.zeros([1], dtype=tf.float32))\n",
    "    \n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    def R2(y_true,y_pred):\n",
    "        u = tf.reduce_sum(tf.square(tf.sub(y_true, y_pred)))\n",
    "        v = tf.reduce_sum(tf.square(tf.sub(y_true, tf.reduce_mean(y_true))))\n",
    "        Rs = 1 - tf.divide(u,v)\n",
    "        return Rs\n",
    "    \n",
    "    def model(data):\n",
    "        multiplications1 = tf.matmul(data, weights1) + biases1\n",
    "        multiplications2 = tf.matmul(multiplications1, weights2) + biases2\n",
    "        multiplications3 = tf.matmul(multiplications2, weights3) + biases3\n",
    "        multiplications4 = tf.matmul(multiplications3, weights4) + biases4\n",
    "        multiplications5 = (tf.matmul(multiplications4, weights5) + biases5)\n",
    "        multiplications6 = (tf.matmul(multiplications5, weights6) + biases6)\n",
    "        multiplications7 = (tf.matmul(multiplications6, weights7) + biases7)\n",
    "        multiplications8 = (tf.matmul(multiplications7, weights8) + biases8)\n",
    "        multiplications9 = (tf.matmul(multiplications8, weights9) + biases9)\n",
    "        multiplications10 = (tf.matmul(multiplications9, weights10) + biases10)\n",
    "        multiplications11 = tf.tanh(tf.matmul(multiplications10, weights11) + biases11)\n",
    "        multiplications12 = tf.tanh(tf.matmul(multiplications11, weights12) + biases12)\n",
    "        multiplications13 = tf.sigmoid(tf.matmul(multiplications12, weights13) + biases13)\n",
    "        multiplications14 = tf.sigmoid(tf.matmul(multiplications13, weights14) + biases14)\n",
    "        output = tf.matmul(multiplications14, weights_out)+biase_out\n",
    "        return output\n",
    "    # Training computation.\n",
    "    # We multiply the inputs with the weight matrix, and add biases. \n",
    "    output = model(tf_train_dataset)\n",
    "    loss = tf.reduce_sum(tf.square(tf.sub(output,tf_train_labels)))\n",
    "                            #+ gamma*tf.reduce_mean(tf.nn.l2_loss(weights_out))+\\\n",
    "                            #gamma*tf.reduce_mean(tf.nn.l2_loss(weights14))\n",
    "                            #gamma*tf.reduce_mean(tf.nn.l2_loss(weights3))+\\\n",
    "                            #gamma*tf.reduce_mean(tf.nn.l2_loss(weights4))+\\\n",
    "                            #gamma*tf.reduce_mean(tf.nn.l2_loss(weights_out))  \n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    \n",
    "    #learning_rate = tf.train.exponential_decay(0.001, global_step, 100000, 0.96)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    #optimizer = tf.train.MOmentumOptimizer(0.01, 0.2).minimize(loss)\n",
    "    \n",
    "    #optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "    optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # These are not part of training, but merely here so that we can report\n",
    "    # accuracy figures as we train.\n",
    "    train_prediction = model(tf_train_dataset)\n",
    "    valid_prediction = model(tf_valid_dataset)\n",
    "    test_prediction = model(tf_test_dataset)\n",
    "    train_r2 = R2(tf_train_labels,train_prediction)\n",
    "    valid_r2 = R2(tf_valid_labels,valid_prediction)\n",
    "    test_r2 = R2(tf_test_labels,test_prediction)\n",
    "    #variables_out ={'w1':weights1,'w2':weights2, 'w3':weights3,  'w4':weights4, 'w5':weights5,'w6':weights6,\\\n",
    "    #                'w7':weights7,'w8':weights8,'w9':weights9,'w10':weights10,'w11':weights11,'w12':weights12,\\\n",
    "    #                'w13':weights13,'w14':weights14, 'wo':weights_out,'b1':biases1,'b2':biases2,'b3':biases3,\\\n",
    "    #                'b4':biases4,'b5':biases5,'b6':biases6,'b7':biases7,'b8':biases8, 'b9':biases9, 'b10':biases10,\\\n",
    "    #                'b11':biases11,'b12':biases12, 'b13':biases13, 'b14':biases14,'bo':biase_out,\\\n",
    "    #                'train_pred': train_prediction,'valid_pred': valid_prediction,'test_pred': test_prediction,\\\n",
    "    #                'train_r2': train_r2, 'valid_r2': valid_r2, 'test_r2': test_r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 285420.406250\n",
      "Training R2:  -860367668.254\n",
      "Validation R2:  -233040523.425\n",
      "================================\n",
      "++++++++++++++++++++++\n",
      "New Best R2:  0.881437\n",
      "Train R2:  0.915684\n",
      "Test R2:  0.879672\n",
      "++++++++++++++++++++++\n",
      "++++++++++++++++++++++\n",
      "New Best R2:  0.882777\n",
      "Train R2:  0.916925\n",
      "Test R2:  0.881391\n",
      "++++++++++++++++++++++\n",
      "++++++++++++++++++++++\n",
      "New Best R2:  0.883857\n",
      "Train R2:  0.918219\n",
      "Test R2:  0.883724\n",
      "++++++++++++++++++++++\n",
      "++++++++++++++++++++++\n",
      "New Best R2:  0.885189\n",
      "Train R2:  0.922867\n",
      "Test R2:  0.887715\n",
      "++++++++++++++++++++++\n",
      "++++++++++++++++++++++\n",
      "New Best R2:  0.886786\n",
      "Train R2:  0.923811\n",
      "Test R2:  0.889087\n",
      "++++++++++++++++++++++\n",
      "++++++++++++++++++++++\n",
      "New Best R2:  0.887994\n",
      "Train R2:  0.924432\n",
      "Test R2:  0.890589\n",
      "++++++++++++++++++++++\n",
      "Loss at step 1000: 0.000024\n",
      "Training R2:  0.926176194892\n",
      "Validation R2:  0.887706548129\n",
      "================================\n",
      "Loss at step 2000: 0.000024\n",
      "Training R2:  0.926172099776\n",
      "Validation R2:  0.887702851212\n",
      "================================\n",
      "Loss at step 3000: 0.000024\n",
      "Training R2:  0.92615569516\n",
      "Validation R2:  0.887694478764\n",
      "================================\n",
      "Loss at step 4000: 0.000360\n",
      "Training R2:  -0.0859532611058\n",
      "Validation R2:  0.506055793191\n",
      "================================\n",
      "Loss at step 5000: 0.000024\n",
      "Training R2:  0.92617513474\n",
      "Validation R2:  0.887700321932\n",
      "================================\n",
      "Loss at step 6000: 0.000024\n",
      "Training R2:  0.926170683645\n",
      "Validation R2:  0.887683379863\n",
      "================================\n",
      "Test R2:  0.892707300025\n",
      "[[ 0.00142987]\n",
      " [ 0.00041828]\n",
      " [ 0.00151867]\n",
      " ..., \n",
      " [ 0.00146131]\n",
      " [ 0.00157356]\n",
      " [ 0.00156833]]\n",
      "+++++++++++++\n",
      "[[ 0.0014196]\n",
      " [ 0.0004934]\n",
      " [ 0.0015292]\n",
      " ..., \n",
      " [ 0.0014644]\n",
      " [ 0.0015723]\n",
      " [ 0.0015703]]\n"
     ]
    }
   ],
   "source": [
    "num_steps = 6001\n",
    "\n",
    "with tf.Session(graph=graph, config=tf.ConfigProto(log_device_placement=True)) as session:\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases. \n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        #offset = (step * batch_size) % (number_of_data - batch_size)\n",
    "        #batch_data = X_train[offset:(offset + batch_size), :]\n",
    "        #batch_labels = y_train[offset:(offset + batch_size)]\n",
    "        #batch_labels = np.reshape(batch_labels, (batch_size,1))\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        #feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions, train_R2,valid_R2, test_R2,Test_prediction = session.run([optimizer, loss,\\\n",
    "                                                                           train_prediction, train_r2,valid_r2,\\\n",
    "                                                                                    test_r2, test_prediction])\n",
    "\n",
    "        if (valid_R2-best_case_R2) > 0.001 :\n",
    "            best_case_R2 = valid_R2\n",
    "            print('++++++++++++++++++++++')\n",
    "            print ('New Best R2: ',best_case_R2)\n",
    "            print ('Train R2: ',train_R2)\n",
    "            print ('Test R2: ',test_R2)\n",
    "            print('++++++++++++++++++++++')\n",
    "            #file_name = 'variables'+str(step)+'.pickle'\n",
    "            test_file_name = 'test'+str(step)+'.txt'\n",
    "            np.savetxt(test_file_name, Test_prediction, delimiter=',')\n",
    "            #pickle.dump(variable_dict, open(file_name, 'wb'))\n",
    "            \n",
    "        if (step % 1000 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training R2: ',Rsquared(y_train, predictions))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph\n",
    "            # dependencies.\n",
    "            print('Validation R2: ',Rsquared( y_validation,valid_prediction.eval()))\n",
    "            print('================================')\n",
    "    print('Test R2: ', Rsquared(y_test, test_prediction.eval()))\n",
    "    print (predictions)\n",
    "    print ('+++++++++++++')\n",
    "    print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
